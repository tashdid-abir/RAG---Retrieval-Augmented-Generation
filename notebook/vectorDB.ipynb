{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0a0cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import document loaders for loading PDF files from directories\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284c7e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DirectoryLoader to load all PDF files with progress tracking\n",
    "document = DirectoryLoader(\n",
    "    \"../data/pdf_files\",\n",
    "    loader_cls=PyPDFLoader,\n",
    "    glob='**/*.pdf',\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494df009",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:06<00:00,  2.31s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load all documents from the directory\n",
    "dir_document = document.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2e2c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import text splitter and define function to chunk documents for better RAG performance\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks for better RAG performance.\n",
    "    \n",
    "    Parameters:\n",
    "    - chunk_size: Maximum characters per chunk (adjust based on your LLM)\n",
    "    - chunk_overlap: Characters to overlap between chunks (preserves context)\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,  # Each chunk: ~1000 characters\n",
    "        chunk_overlap=chunk_overlap,  # 200 chars overlap for context continuity\n",
    "        length_function=len,  # How to measure length\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Try paragraph, line, word, then character splits\n",
    "    )\n",
    "    # Actually split the documents\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show what a chunk looks like\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2117f890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 170 documents into 720 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: . \n",
      ". \n",
      "Latest updates: hps://dl.acm.org/doi/10.1145/3773084\n",
      ". \n",
      ". \n",
      "RESEARCH-ARTICLE\n",
      "Large Language Models for Constructing and Optimizing Machine\n",
      "Learning Workflows: A Survey\n",
      "YANG GU, Shanghai Jiao Ton...\n",
      "Metadata: {'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-21T22:02:52-08:00', 'moddate': '2026-01-21T22:02:53-08:00', 'subject': 'ACM Trans. Softw. Eng. Methodol. 0.0', 'title': 'Large Language Models for Constructing and Optimizing Machine Learning Workflows: A Survey', 'source': '..\\\\data\\\\pdf_files\\\\Constructing and Optimizing Machine.pdf', 'total_pages': 45, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# Split documents into chunks with specified size and overlap\n",
    "chunked_documents = split_documents(dir_document, chunk_size=1000, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a12c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-21T22:02:52-08:00', 'moddate': '2026-01-21T22:02:53-08:00', 'subject': 'ACM Trans. Softw. Eng. Methodol. 0.0', 'title': 'Large Language Models for Constructing and Optimizing Machine Learning Workflows: A Survey', 'source': '..\\\\data\\\\pdf_files\\\\Constructing and Optimizing Machine.pdf', 'total_pages': 45, 'page': 0, 'page_label': '1'}, page_content='. \\n. \\nLatest updates: h\\ue03cps://dl.acm.org/doi/10.1145/3773084\\n. \\n. \\nRESEARCH-ARTICLE\\nLarge Language Models for Constructing and Optimizing Machine\\nLearning Workflows: A Survey\\nYANG GU, Shanghai Jiao Tong University, Shanghai, China\\n. \\nHENGYU YOU, Shanghai Jiao Tong University, Shanghai, China\\n. \\nJIAN CAO, Shanghai Jiao Tong University, Shanghai, China\\n. \\nMURAN YU, Stanford University, Stanford, CA, United States\\n. \\nHAORAN FAN, Shanghai Jiao Tong University, Shanghai, China\\n. \\nSHIYOU QIAN, Shanghai Jiao Tong University, Shanghai, China\\n. \\n. \\n. \\nOpen Access Support provided by:\\n. \\nShanghai Jiao Tong University\\n. \\nStanford University\\n. \\nPDF Download\\n3773084.pdf\\n21 January 2026\\nTotal Citations: 3\\nTotal Downloads: 488\\n. \\n. \\nAccepted: 20 October 2025\\nRevised: 23 August 2025\\nReceived: 11 February 2025\\n. \\n. \\nCitation in BibTeX format\\n. \\n. \\nACM Transactions on So\\ue039ware Engineering and Methodology\\nh\\ue03cps://doi.org/10.1145/3773084\\nEISSN: 1557-7392\\n.')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the first document\n",
    "doc = dir_document[0]\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dec8dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the document object type\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5822b7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document subject : ACM Trans. Softw. Eng. Methodol. 0.0\n",
      "\n",
      "\n",
      "Document content : . \n",
      ". \n",
      "Latest updates: hps://dl.acm.org/doi/10.1145/3773084\n",
      ". \n",
      ". \n",
      "RESEARCH-ARTICLE\n",
      "Large Language Models for Constructing and Optimizing Machine\n",
      "Learning Workflows: A Survey\n",
      "YANG GU, Shanghai Jiao Tong University, Shanghai, China\n",
      ". \n",
      "HENGYU YOU, Shanghai Jiao Tong University, Shanghai, China\n",
      ". \n",
      "JIAN CAO, Shanghai Jiao Tong University, Shanghai, China\n",
      ". \n",
      "MURAN YU, Stanford University, Stanford, CA, United States\n",
      ". \n",
      "HAORAN FAN, Shanghai Jiao Tong University, Shanghai, China\n",
      ". \n",
      "SHIYOU QIAN, Shanghai\n"
     ]
    }
   ],
   "source": [
    "# Display document metadata and content preview\n",
    "print(f\"Document subject : {doc.metadata.get('subject')}\")\n",
    "print(f\"\\n\\nDocument content : {doc.page_content[:500]}\")  # First 500 chars of content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769f5a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for embeddings generation\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict, Any, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0c2f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model : all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 869.08it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model loaded with dimension : 384\n"
     ]
    }
   ],
   "source": [
    "# Load the sentence transformer model for generating embeddings\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "print(f\"Loading model : {model_name}\")\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "model_dimension = model.get_sentence_embedding_dimension()\n",
    "print(f\"\\nModel loaded with dimension : {model_dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ddb407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 23/23 [00:25<00:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape : (720, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.04726338,  0.00032324,  0.0190388 , ...,  0.00690563,\n",
       "         0.00175409,  0.05870934],\n",
       "       [-0.026851  , -0.00267686,  0.02108338, ...,  0.01690894,\n",
       "         0.02223114,  0.03617563],\n",
       "       [-0.00562933, -0.04167898,  0.00799582, ...,  0.05661926,\n",
       "         0.10684081,  0.01380027],\n",
       "       ...,\n",
       "       [-0.11117143,  0.0337267 , -0.0059452 , ...,  0.0478947 ,\n",
       "         0.02891634,  0.07149113],\n",
       "       [-0.216594  , -0.03837626, -0.04161664, ...,  0.02678554,\n",
       "        -0.00071634,  0.10696496],\n",
       "       [ 0.02199661, -0.0140572 , -0.01843296, ...,  0.07923552,\n",
       "         0.07359981,  0.02747879]], shape=(720, 384), dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate embeddings for all chunked documents\n",
    "print(f\"Generating embedding...\")\n",
    "embeddings = model.encode([d.page_content for d in chunked_documents], show_progress_bar=True)  # Convert text chunks to vectors\n",
    "print(f\"Generated embeddings with shape : {embeddings.shape}\")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a38a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for ChromaDB vector store management\n",
    "import os\n",
    "import chromadb\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468d563a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing collection\n",
      "Vector store initialized. Collection : Collection(name=pdf_documents)\n",
      "Existing documents in collection : 0\n",
      "Adding 720 documents to vector store...\n",
      "Successfully added 720 documents to vector store\n",
      "Total documents in collection : 720\n"
     ]
    }
   ],
   "source": [
    "# Define VectorStore class to manage ChromaDB operations and add documents with embeddings\n",
    "class VectorStore:\n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\") -> None:\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "    \n",
    "    def _initialize_store(self) -> None:\n",
    "        try:\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)  # Create persistent client for disk storage\n",
    "\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name = self.collection_name,\n",
    "                metadata={\"description\":\"PDF document embedding for RAG\"}\n",
    "            )\n",
    "\n",
    "            print(f\"Vector store initialized. Collection : {self.collection}\")\n",
    "            print(f\"Existing documents in collection : {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "    \n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of embedding must be same to number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"  # Generate unique ID with UUID prefix\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            metadata = dict(doc.metadata)  # Copy existing metadata\n",
    "            metadata['doc_index'] = i  # Add index for tracking\n",
    "            metadata['content_length'] = len(doc.page_content)  # Store chunk size\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            documents_text.append(doc.page_content)\n",
    "\n",
    "            embeddings_list.append(embedding.tolist())  # Convert numpy array to list for ChromaDB\n",
    "        \n",
    "        try: \n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection : {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding document to vector store: {e}\")\n",
    "\n",
    "\n",
    "# Clean up existing collection and initialize VectorStore with documents and embeddings\n",
    "temp_client = chromadb.PersistentClient(path=\"../data/vector_store\")\n",
    "try:\n",
    "    temp_client.delete_collection(name=\"pdf_documents\")  # Remove old collection to avoid duplicates\n",
    "    print(\"Deleted existing collection\")\n",
    "except:\n",
    "    print(\"No existing collection to delete\")\n",
    "\n",
    "vectorstore = VectorStore()\n",
    "vectorstore.add_documents(chunked_documents, embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
